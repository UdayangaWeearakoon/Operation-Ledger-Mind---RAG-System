{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d531e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Working directory: d:\\Bootcamps\\AEE 2026 Jan\\Mini Projects\\Operation Ledger Mind\n",
      "{'OPENAI_API_KEY': True, 'OPENROUTER_API_KEY': True, 'GROQ_API_KEY': False, 'GOOGLE_API_KEY': False, 'COHERE_API_KEY': False}\n",
      "‚úÖ Config loaded:\n",
      "  LLM: openai / gpt-4o-mini\n",
      "  Embeddings: sbert / sentence-transformers/all-MiniLM-L6-v2\n",
      "  Temperature: 0.2\n",
      "  Artifacts: ./artifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Bootcamps\\AEE 2026 Jan\\Mini Projects\\Operation Ledger Mind\\src\\services\\llm_services.py:375: UserWarning: ‚ö†Ô∏è  GROQ_API_KEY not found in environment\n",
      "  warnings.warn(f\"‚ö†Ô∏è  {key} not found in environment\")\n",
      "d:\\Bootcamps\\AEE 2026 Jan\\Mini Projects\\Operation Ledger Mind\\src\\services\\llm_services.py:375: UserWarning: ‚ö†Ô∏è  GOOGLE_API_KEY not found in environment\n",
      "  warnings.warn(f\"‚ö†Ô∏è  {key} not found in environment\")\n",
      "d:\\Bootcamps\\AEE 2026 Jan\\Mini Projects\\Operation Ledger Mind\\src\\services\\llm_services.py:375: UserWarning: ‚ö†Ô∏è  COHERE_API_KEY not found in environment\n",
      "  warnings.warn(f\"‚ö†Ô∏è  {key} not found in environment\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Need this to run notebooks from the notebooks/ directory, else can't find src/\n",
    "# Get the notebook's current directory and find project root\n",
    "notebook_dir = Path.cwd()\n",
    "if notebook_dir.name == \"notebooks\":\n",
    "    project_root = notebook_dir.parent\n",
    "else:\n",
    "    project_root = notebook_dir\n",
    "\n",
    "os.chdir(project_root)\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
    "\n",
    "from src.services.llm_services import (\n",
    "    load_config,\n",
    "    get_llm,\n",
    "    get_text_embeddings,\n",
    "    validate_api_keys,\n",
    "    print_config_summary\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "config = load_config(\"src/config/config.yaml\")\n",
    "\n",
    "from utils.llm_client import LLMClient\n",
    "\n",
    "# Initialize LLM client (LLM Configuration)\n",
    "client = LLMClient(provider=config[\"llm_provider\"].lower(), model=config.get(\"openrouter_model\", config.get(\"llm_model\")))\n",
    "\n",
    "# Validate API keys\n",
    "print(validate_api_keys(config, verbose=True))\n",
    "\n",
    "# Print summary\n",
    "print_config_summary(config)\n",
    "\n",
    "# Ensure output directory exists\n",
    "Path(config[\"output_dir\"]).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de8397e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Bootcamps\\AEE 2026 Jan\\Mini Projects\\Operation Ledger Mind\\src\\services\\llm_services.py:129: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  return HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM: openai / gpt-4o-mini\n",
      "‚úÖ Embeddings: sentence-transformers/all-MiniLM-L6-v2\n",
      "‚úÖ Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "\n",
      "üîç Testing LLM API connection...\n",
      "‚úÖ LLM API verified: API working!\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM, Embeddings, and Reranker\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "llm = get_llm(config)\n",
    "embeddings = get_text_embeddings(config)\n",
    "\n",
    "# CrossEncoder: A reranker model that scores query-document pairs\n",
    "# Unlike bi-encoders (embeddings), cross-encoders see query AND document together\n",
    "# Gives higher accuracy but is slower (can't pre-compute embeddings)\n",
    "\n",
    "reranker = CrossEncoder(\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # Model trained on MS MARCO dataset\n",
    "                                             # Other options: \"cross-encoder/ms-marco-TinyBERT-L-2-v2\" (faster)\n",
    "                                             #                \"cross-encoder/ms-marco-MiniLM-L-12-v2\" (more accurate)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LLM: {config['llm_provider']} / {config['llm_model']}\")\n",
    "print(f\"‚úÖ Embeddings: {config['text_emb_model']}\")\n",
    "print(f\"‚úÖ Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "# Verify API key with test completion\n",
    "print(\"\\nüîç Testing LLM API connection...\")\n",
    "try:\n",
    "    test_response = llm.invoke(\"Say 'API working!' if you can read this.\")\n",
    "    test_msg = test_response.content if hasattr(test_response, 'content') else str(test_response)\n",
    "    print(f\"‚úÖ LLM API verified: {test_msg[:50]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LLM API test failed: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Please check your .env file and API key configuration.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919a298",
   "metadata": {},
   "source": [
    "PDF Load + Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348ba0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loading PDF: ./data/raw/Uber_annual_report_2024.pdf\n",
      "    Total pages: 142\n",
      "‚úÖ Extracted 624,112 characters from 142 pages\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def load_and_clean_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load PDF and clean by removing common headers/footers.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text content\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages = []\n",
    "    \n",
    "    \n",
    "    print(f\"üìÑ Loading PDF: {pdf_path}\")\n",
    "    print(f\"    Total pages: {len(reader.pages)}\")\n",
    "    \n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        \n",
    "        # Remove common headers/footers\n",
    "        text = text.replace(\"Uber Technologies, Inc.\", \"\")\n",
    "        text = text.replace(\"2024 Annual Report\", \"\")\n",
    "        text = text.replace(\"Form 10-K\", \"Form 10-K\")  # Keep Form 10-K as it's important\n",
    "        \n",
    "        # Remove excessive whitespace\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        pages.append(text)\n",
    "    \n",
    "    full_text = \"\\n\\n\".join(pages)\n",
    "    print(f\"‚úÖ Extracted {len(full_text):,} characters from {len(pages)} pages\")\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "# Load the document\n",
    "document_text = load_and_clean_pdf(config[\"data_root\"] + \"/Uber_annual_report_2024.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b6c166",
   "metadata": {},
   "source": [
    "Chunking 1500 chars\n",
    "\n",
    "use fixed chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4cd3346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Results:\n",
      "\tTotal chunks: 544\n",
      "\tAvg chunk size: 1209 chars\n",
      "\tMin/Max: 25 / 1500 chars\n",
      "\n",
      "üìù Sample Chunk:\n",
      "   Length: 610 chars\n",
      "   Preview: On Our Way 2024 ANNUAL REPORT\n",
      "\n",
      "Uber‚Äôs Mission We reimagine the way the world moves for the better We are Uber. The go-getters. The kind of people who are relentless about our mission to help people go anywhere and get anything and earn their way. Movement is what we power. It‚Äôs our lifeblood. It run...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from rpds import List\n",
    "\n",
    "def create_chunks(text: str, chunk_size: int = 1500, chunk_overlap: int = 200) -> list:\n",
    "    \"\"\"\n",
    "    Split document into semantically meaningful chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: Full document text\n",
    "        chunk_size: Target size of each chunk, if not provide give a value of 1500\n",
    "        chunk_overlap: Overlap between chunks to maintain context, if not provide give a value of 200\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    print(f\"Chunking Results:\")\n",
    "    print(f\"\\tTotal chunks: {len(chunks)}\")\n",
    "    print(f\"\\tAvg chunk size: {sum(len(c) for c in chunks) // len(chunks)} chars\")\n",
    "    print(f\"\\tMin/Max: {min(len(c) for c in chunks)} / {max(len(c) for c in chunks)} chars\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create chunks\n",
    "chunks = create_chunks(document_text, config[\"chunk_size\"], config[\"chunk_overlap\"])\n",
    "\n",
    "# Display sample chunk\n",
    "print(f\"\\nüìù Sample Chunk:\")\n",
    "print(f\"   Length: {len(chunks[0])} chars\")\n",
    "print(f\"   Preview: {chunks[0][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e23fba",
   "metadata": {},
   "source": [
    "Master prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9b3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP A: Question Generation Prompt (LLM A)\n",
    "QUESTION_GENERATION_PROMPT = \"\"\"You are a financial analyst creating training questions for a model that will answer questions about Uber's 2024 Annual Report.\n",
    "\n",
    "Based on the following text excerpt, generate EXACTLY 10 diverse questions that cover:\n",
    "\n",
    "**Category Distribution:**\n",
    "- 4 Hard Facts questions: Specific numbers, dates, percentages, financial metrics, concrete data points\n",
    "- 3 Strategic Summary questions: Business strategies, competitive advantages, risk factors, market analysis\n",
    "- 3 Stylistic/Creative questions: Trends analysis, future implications, comparative insights, interpretations\n",
    "\n",
    "**Context:**\n",
    "{chunk}\n",
    "\n",
    "**Instructions:**\n",
    "1. Questions must be answerable ONLY from the provided context\n",
    "2. Be specific - reference exact metrics, dates, or concepts from the text\n",
    "3. Vary question complexity and depth\n",
    "4. Use natural language (avoid overly formal phrasing)\n",
    "\n",
    "**Output Format (JSON):**\n",
    "[\n",
    "  {{\"category\": \"Hard Facts\", \"question\": \"What was Uber's total revenue in Q4 2024?\"}},\n",
    "  {{\"category\": \"Strategic Summary\", \"question\": \"How is Uber addressing regulatory challenges in European markets?\"}},\n",
    "  {{\"category\": \"Stylistic/Creative\", \"question\": \"What does the shift toward autonomous vehicles suggest about Uber's long-term vision?\"}}\n",
    "]\n",
    "\n",
    "Generate the 10 questions now:\"\"\"\n",
    "\n",
    "# STEP B: Answer Generation Prompt (LLM B)\n",
    "ANSWER_GENERATION_PROMPT = \"\"\"You are an expert financial analyst providing precise answers based on Uber's 2024 Annual Report.\n",
    "\n",
    "**Context:**\n",
    "{chunk}\n",
    "\n",
    "**Question:**\n",
    "{question}\n",
    "\n",
    "**Instructions:**\n",
    "1. Answer ONLY based on the information in the context above\n",
    "2. Be specific - cite exact numbers, dates, and details when available\n",
    "3. If the context doesn't contain enough information, say \"Based on the provided context...\"\n",
    "4. For Hard Facts: Provide concise, data-driven answers\n",
    "5. For Strategic/Creative: Provide thoughtful analysis while staying grounded in the text\n",
    "6. Keep answers between 2-5 sentences (50-150 words)\n",
    "\n",
    "Provide your answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e021b9",
   "metadata": {},
   "source": [
    "Generate Q/A (LLM A ‚Üí LLM B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a3d123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing 20 chunks...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5f50a3f6d04f5f96a88993b76f4141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Q/A pairs:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "Generated 10 valid questions\n",
      "\n",
      "‚úÖ Generated 200 Q/A pairs from 20 chunks\n",
      "\n",
      "üìä Category Distribution:\n",
      "   Hard Facts: 80 (40.0%)\n",
      "   Strategic Summary: 60 (30.0%)\n",
      "   Stylistic/Creative: 60 (30.0%)\n"
     ]
    }
   ],
   "source": [
    "def generate_questions(chunk: str) -> list[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Generate 10 questions from a chunk using the LLMClient with expect_json.\n",
    "    \"\"\"\n",
    "    prompt = QUESTION_GENERATION_PROMPT.format(chunk=chunk)\n",
    "    \n",
    "    response = client.chat(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        expect_json=True,           # tells client to try to parse JSON\n",
    "        temperature=0.7,\n",
    "        max_tokens=1200,\n",
    "    )\n",
    "\n",
    "    # ‚îÄ‚îÄ Handle the response ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if response.get(\"parse_error\"):\n",
    "        print(f\"‚ö†Ô∏è JSON parse error in chunk: {response['parse_error']}\")\n",
    "        print(f\"Raw text snippet: {response['text'][:400]}...\\n\")\n",
    "        return []\n",
    "\n",
    "    parsed = response.get(\"parsed\")\n",
    "    \n",
    "    if parsed is None:\n",
    "        print(\"‚ö†Ô∏è No parsed JSON returned\")\n",
    "        print(f\"Raw text: {response['text'][:400]}...\\n\")\n",
    "        return []\n",
    "\n",
    "    if not isinstance(parsed, list):\n",
    "        print(f\"‚ö†Ô∏è Parsed content is not a list: {type(parsed)}\")\n",
    "        return []\n",
    "\n",
    "    # Validate structure (optional but recommended)\n",
    "    valid_questions = []\n",
    "    for item in parsed:\n",
    "        if isinstance(item, dict) and \"question\" in item and \"category\" in item:\n",
    "            valid_questions.append(item)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Invalid question format: {item}\")\n",
    "\n",
    "    print(f\"Generated {len(valid_questions)} valid questions\")\n",
    "    return valid_questions[:10]\n",
    "\n",
    "def generate_answer(chunk: str, question_data: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Generate answer using LLM B.\n",
    "    \"\"\"\n",
    "    prompt = ANSWER_GENERATION_PROMPT.format(\n",
    "        chunk=chunk,\n",
    "        question=question_data[\"question\"]\n",
    "    )\n",
    "    \n",
    "    response = client.chat(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        expect_json=False,           # answers are free text, not JSON\n",
    "        temperature=0.3,             # lower for factual answers\n",
    "        max_tokens=400,\n",
    "    )\n",
    "    \n",
    "    if \"parse_error\" in response and response[\"parse_error\"]:\n",
    "        print(f\"Warning: parse error in answer - using raw text\")\n",
    "    \n",
    "    # Always take the \"text\" field\n",
    "    answer_text = response.get(\"text\", \"\").strip()\n",
    "    \n",
    "    if not answer_text:\n",
    "        return \"No answer could be generated from the provided context.\"\n",
    "    \n",
    "    return answer_text\n",
    "\n",
    "def process_all_chunks(chunks: list[str], questions_per_chunk: int = 10) -> list[Dict]:\n",
    "    \"\"\"\n",
    "    Main generation loop: Process all chunks and create Q/A pairs.\n",
    "    \"\"\"\n",
    "    all_qa_pairs = []\n",
    "    \n",
    "    # Limit to first 20 chunks for demo (remove limit for full run)\n",
    "    chunks_to_process = chunks[:20]  # Change to chunks[:] for full dataset\n",
    "    \n",
    "    print(f\"\\nüîÑ Processing {len(chunks_to_process)} chunks...\\n\")\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(tqdm(chunks_to_process, desc=\"Generating Q/A pairs\")):\n",
    "        try:\n",
    "            # Step A: Generate questions\n",
    "            questions = generate_questions(chunk)\n",
    "            \n",
    "            if not questions:\n",
    "                print(f\"‚ö†Ô∏è Skipping chunk {chunk_idx} - no questions generated\")\n",
    "                continue\n",
    "            \n",
    "            # Step B: Generate answers for each question\n",
    "            for q_data in questions[:questions_per_chunk]:\n",
    "                try:\n",
    "                    answer = generate_answer(chunk, q_data)\n",
    "                    \n",
    "                    all_qa_pairs.append({\n",
    "                        \"chunk_id\": chunk_idx,\n",
    "                        \"category\": q_data.get(\"category\", \"Unknown\"),\n",
    "                        \"question\": q_data[\"question\"],\n",
    "                        \"answer\": answer,\n",
    "                        \"context\": chunk\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error generating answer for chunk {chunk_idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing chunk {chunk_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Generated {len(all_qa_pairs)} Q/A pairs from {len(chunks_to_process)} chunks\")\n",
    "    \n",
    "    # Display statistics\n",
    "    categories = {}\n",
    "    for pair in all_qa_pairs:\n",
    "        cat = pair[\"category\"]\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìä Category Distribution:\")\n",
    "    for cat, count in categories.items():\n",
    "        print(f\"   {cat}: {count} ({count/len(all_qa_pairs)*100:.1f}%)\")\n",
    "    \n",
    "    return all_qa_pairs\n",
    "\n",
    "# Run the generation loop\n",
    "qa_pairs = process_all_chunks(chunks, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b52360a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully saved 200 Q/A pairs to data\\generated\\generated_data.csv\n",
      "\n",
      "First 3 saved entries:\n",
      "  - Hard Facts | Q: What is Uber's mission statement as outlined in the 2024 Annual Report?...\n",
      "  - Hard Facts | Q: What key performance indicator is described as Uber's 'lifeblood'?...\n",
      "  - Hard Facts | Q: In what year did Uber aim to reimagine the way the world moves according to thei...\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "output_file = Path(config[\"generated_dir\"]) / \"generated_data.csv\"\n",
    "\n",
    "fieldnames = [\n",
    "    \"chunk_id\",\n",
    "    \"category\",\n",
    "    \"question\",\n",
    "    \"answer\",\n",
    "    \"context\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write all rows\n",
    "        for pair in qa_pairs:\n",
    "            writer.writerow(pair)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully saved {len(qa_pairs)} Q/A pairs to {output_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Failed to save CSV: {e}\")\n",
    "\n",
    "# Optional: also show a quick preview of first few rows\n",
    "if qa_pairs:\n",
    "    print(\"\\nFirst 3 saved entries:\")\n",
    "    for pair in qa_pairs[:3]:\n",
    "        print(f\"  - {pair['category']} | Q: {pair['question'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064f960",
   "metadata": {},
   "source": [
    "Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c18f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Dataset Split:\n",
      "   Training set: 160 pairs (80.0%)\n",
      "   Test set: 40 pairs (20.0%)\n",
      "‚úÖ Saved 160 items to data\\processed\\train.jsonl\n",
      "‚úÖ Saved 40 items to data\\processed\\golden_test_set.jsonl\n",
      "\n",
      "‚úÖ Data factory complete!\n",
      "   üìÅ Training data: data\\processed\\train.jsonl\n",
      "   üìÅ Test data: data\\processed\\golden_test_set.jsonl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def save_jsonl(data: list[Dict], filepath: str):\n",
    "    \"\"\"\n",
    "    Save data in JSONL format (one JSON object per line).\n",
    "    \"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    print(f\"‚úÖ Saved {len(data)} items to {filepath}\")\n",
    "\n",
    "# Split data 80/20\n",
    "train_data, test_data = train_test_split(\n",
    "    qa_pairs,\n",
    "    test_size=1 - config[\"train_ratio\"],\n",
    "    random_state=42,\n",
    "    stratify=[pair[\"category\"] for pair in qa_pairs]  # Maintain category distribution\n",
    ")\n",
    "\n",
    "print(f\"\\nüì¶ Dataset Split:\")\n",
    "print(f\"   Training set: {len(train_data)} pairs ({len(train_data)/len(qa_pairs)*100:.1f}%)\")\n",
    "print(f\"   Test set: {len(test_data)} pairs ({len(test_data)/len(qa_pairs)*100:.1f}%)\")\n",
    "\n",
    "# Save to JSONL files\n",
    "train_path = Path(config[\"output_dir\"]) / \"train.jsonl\"\n",
    "test_path = Path(config[\"output_dir\"]) / \"golden_test_set.jsonl\"\n",
    "\n",
    "save_jsonl(train_data, str(train_path))\n",
    "save_jsonl(test_data, str(test_path))\n",
    "\n",
    "print(f\"\\n‚úÖ Data factory complete!\")\n",
    "print(f\"   üìÅ Training data: {train_path}\")\n",
    "print(f\"   üìÅ Test data: {test_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Operation Ledger Mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
