{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8db1d2b9",
   "metadata": {},
   "source": [
    "# Part 4: The Showdown (Evaluation Arena)\n",
    "\n",
    "Which model is better? Cost vs. Accuracy.\n",
    "\n",
    "**Goal:** Run Golden Test Set through BOTH models (Fine-tuned vs RAG).\n",
    "\n",
    "**Metrics:**\n",
    "1. **ROUGE-L**: Textual overlap with ground truth.\n",
    "2. **LLM-as-a-Judge**: \"Faithfulness\" and \"Accuracy\" (1-5 scale).\n",
    "3. **Latency**: Time to generate response (ms).\n",
    "\n",
    "**Bonus:** Cost Comparison (500 daily users * 10 queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup_env",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Working directory: d:\\Bootcamps\\AEE 2026 Jan\\Mini Projects\\Operation Ledger Mind\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from rouge_score import rouge_scorer\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Setup paths\n",
    "notebook_dir = Path.cwd()\n",
    "if notebook_dir.name == \"notebooks\":\n",
    "    project_root = notebook_dir.parent\n",
    "else:\n",
    "    project_root = notebook_dir\n",
    "\n",
    "os.chdir(project_root)\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"ðŸ“‚ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Load config\n",
    "from src.services.llm_services import load_config\n",
    "config = load_config(\"src/config/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## 1. Load Golden Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_test_set",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 240 test cases from data\\processed\\golden_test_set.jsonl\n",
      "Sample: What can be inferred about the future relationship between Uber and its Drivers based on the current challenges mentioned?\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(filepath: str) -> List[Dict]:\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "test_file = Path(config[\"output_dir\"]) / \"golden_test_set.jsonl\"\n",
    "test_set = load_jsonl(test_file)\n",
    "\n",
    "print(f\"âœ… Loaded {len(test_set)} test cases from {test_file}\")\n",
    "if test_set:\n",
    "    print(f\"Sample: {test_set[0]['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_rag",
   "metadata": {},
   "source": [
    "## 2. Setup RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rag_setup_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.connect import ConnectionParams\n",
    "from sentence_transformers import CrossEncoder\n",
    "from openai import OpenAI\n",
    "from collections import defaultdict\n",
    "\n",
    "# Weaviate Client\n",
    "client = weaviate.WeaviateClient(\n",
    "    connection_params=ConnectionParams.from_url(\"http://localhost:8080\", grpc_port=50051)\n",
    ")\n",
    "client.connect()\n",
    "col = client.collections.get(\"FinancialChunk\")\n",
    "\n",
    "# Reranker\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
    "\n",
    "# OpenAI Client\n",
    "oa = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# RAG Functions\n",
    "def retrieve_bm25(q: str, k: int = 25):\n",
    "    res = col.query.bm25(query=q, limit=k)\n",
    "    return res.objects\n",
    "\n",
    "def retrieve_vector(q: str, k: int = 25):\n",
    "    res = col.query.near_text(query=q, limit=k)\n",
    "    return res.objects\n",
    "\n",
    "def rrf_fuse(lists, rrf_k=60, top_n=30):\n",
    "    scores = defaultdict(float)\n",
    "    for lst in lists:\n",
    "        for rank, obj in enumerate(lst, start=1):\n",
    "            scores[str(obj.uuid)] += 1.0 / (rrf_k + rank)\n",
    "    ranked_ids = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    wanted = {rid for rid, _ in ranked_ids}\n",
    "    by_id = {}\n",
    "    for lst in lists:\n",
    "        for obj in lst:\n",
    "            if str(obj.uuid) in wanted and str(obj.uuid) not in by_id:\n",
    "                by_id[str(obj.uuid)] = obj\n",
    "    return [by_id[rid] for rid, _ in ranked_ids if rid in by_id]\n",
    "\n",
    "def cross_encoder_rerank(question, candidates, top_n=8):\n",
    "    pairs = [(question, c.properties[\"text\"]) for c in candidates]\n",
    "    if not pairs:\n",
    "        return []\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [c for c, _ in ranked[:top_n]]\n",
    "\n",
    "def build_context(objs):\n",
    "    out = []\n",
    "    for i, o in enumerate(objs, start=1):\n",
    "        out.append(f\"[{i}] (p.{o.properties.get('page')})\\n{o.properties.get('text')}\")\n",
    "    return \"\\n\\n\".join(out)\n",
    "\n",
    "def query_rag(question: str) -> Dict[str, Any]:\n",
    "    t0 = time.time()\n",
    "    bm25 = retrieve_bm25(question)\n",
    "    vec = retrieve_vector(question)\n",
    "    fused = rrf_fuse([bm25, vec])\n",
    "    top = cross_encoder_rerank(question, fused)\n",
    "    context = build_context(top)\n",
    "    \n",
    "    system = \"You are a financial assistant. Answer using the provided context.\"\n",
    "    user_msg = f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    \n",
    "    resp = oa.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user_msg}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    latency = int((time.time() - t0) * 1000)\n",
    "    return {\"answer\": resp.choices[0].message.content, \"latency\": latency}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_ft",
   "metadata": {},
   "source": [
    "## 3. Setup Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup of the fine tuned model and the running of the main evaluation loop for it are done in the setup_ft.ipynb file. FT results are saved in generated data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_metrics",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics (ROUGE + LLM Judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "metrics_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def calculate_rouge(pred: str, ref: str) -> float:\n",
    "    scores = rouge.score(ref, pred)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "def llm_judge(question: str, ground_truth: str, prediction: str) -> int:\n",
    "    if not prediction or not prediction.strip():\n",
    "        return 1\n",
    "        \n",
    "    prompt = f\"\"\"You are an impartial judge evaluating the quality of an AI-generated answer.\n",
    "    \n",
    "    Question: {question}\n",
    "    Ground Truth: {ground_truth}\n",
    "    Prediction: {prediction}\n",
    "    \n",
    "    Rate the Prediction on a scale of 1 to 5 based on Faithfulness and Accuracy compared to the Ground Truth.\n",
    "    1 = Completely wrong or irrelevant\n",
    "    3 = Partially correct but missing details\n",
    "    5 = Perfect match in information and intent\n",
    "    \n",
    "    Return ONLY the integer score (1-5).\"\"\"\n",
    "    \n",
    "    try:\n",
    "        resp = oa.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        score = int(resp.choices[0].message.content.strip())\n",
    "        return max(1, min(5, score))\n",
    "    except:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_eval_loop",
   "metadata": {},
   "source": [
    "## 5. Main Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eval_loop_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting evaluation on 50 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [07:18<00:00,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluation Complete! Saved: eval_rag_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "limit = 50\n",
    "\n",
    "ENABLE_JUDGE = True\n",
    "\n",
    "print(f\"ðŸš€ Starting evaluation on {min(len(test_set), limit)} samples...\")\n",
    "\n",
    "for i, item in enumerate(tqdm(test_set[:limit], desc=\"Eval\")):\n",
    "    q = item[\"question\"]\n",
    "    gt = item[\"answer\"]\n",
    "    category = item.get(\"category\", \"Unknown\")\n",
    "\n",
    "    row = {\"question\": q, \"category\": category, \"original_answer\": gt}\n",
    "\n",
    "    # --- RAG ---\n",
    "    try:\n",
    "        rag_res = query_rag(q)\n",
    "        rag_ans = rag_res[\"answer\"]\n",
    "        rag_lat = rag_res[\"latency\"]\n",
    "        rag_rouge = calculate_rouge(rag_ans, gt)\n",
    "        rag_score = llm_judge(q, gt, rag_ans) if ENABLE_JUDGE else None\n",
    "    except Exception as e:\n",
    "        rag_ans, rag_lat, rag_rouge, rag_score = \"ERROR\", None, None, None\n",
    "\n",
    "    row.update({\n",
    "        \"RAG_Answer\": rag_ans,\n",
    "        \"RAG_Latency\": rag_lat,\n",
    "        \"RAG_ROUGE\": rag_rouge,\n",
    "        \"RAG_Score\": rag_score\n",
    "    })\n",
    "\n",
    "    results.append(row)\n",
    "\n",
    "    # Save partial every 5 rows\n",
    "    if (i + 1) % 5 == 0:\n",
    "        pd.DataFrame(results).to_csv(\"data/generated/eval_rag_partial.csv\", index=False)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"data/generated/eval_rag_results.csv\", index=False)\n",
    "print(\"âœ… Evaluation Complete! Saved: eval_rag_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_analysis",
   "metadata": {},
   "source": [
    "## 6. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "analysis_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loading results from:\n",
      " - data/generated/eval_rag_results.csv\n",
      " - data/generated/eval_ft_results.csv\n",
      "ðŸ“Š Overall Summary:\n",
      "RAG_Latency     7945.940000\n",
      "RAG_ROUGE          0.292667\n",
      "RAG_Score          4.540000\n",
      "FT_Latency     12567.660000\n",
      "FT_ROUGE           0.303838\n",
      "FT_Score           1.000000\n",
      "dtype: float64\n",
      "\n",
      "ðŸ“Š By Category (Score):\n",
      "                    RAG_Score  FT_Score\n",
      "category                               \n",
      "Hard Facts           4.291667       1.0\n",
      "Strategic Summary    5.000000       1.0\n",
      "Stylistic/Creative   4.625000       1.0\n"
     ]
    }
   ],
   "source": [
    "# Load results\n",
    "rag_results_path = \"data/generated/eval_rag_results.csv\"\n",
    "ft_results_path = \"data/generated/eval_ft_results.csv\"\n",
    "\n",
    "if os.path.exists(rag_results_path) and os.path.exists(ft_results_path):\n",
    "    print(f\"âœ… Loading results from:\\n - {rag_results_path}\\n - {ft_results_path}\")\n",
    "    rag_df = pd.read_csv(rag_results_path)\n",
    "    ft_df = pd.read_csv(ft_results_path)\n",
    "\n",
    "    # Merge dataframes\n",
    "    # Select columns from FT to avoid duplicates\n",
    "    ft_cols = ['question', 'FT_Latency', 'FT_ROUGE', 'FT_Score']\n",
    "    if 'FT_Answer' in ft_df.columns: ft_cols.append('FT_Answer')\n",
    "    \n",
    "    df = pd.merge(rag_df, ft_df[ft_cols], on='question', how='inner')\n",
    "    \n",
    "    if not df.empty:\n",
    "        # Average Metrics\n",
    "        metrics = [\"RAG_Latency\", \"RAG_ROUGE\", \"RAG_Score\", \"FT_Latency\", \"FT_ROUGE\", \"FT_Score\"]\n",
    "        existing = [m for m in metrics if m in df.columns]\n",
    "        \n",
    "        print(\"ðŸ“Š Overall Summary:\")\n",
    "        print(df[existing].mean())\n",
    "\n",
    "        # Check if we have valid scores\n",
    "        has_rag_score = \"RAG_Score\" in df.columns and df[\"RAG_Score\"].notna().any()\n",
    "        has_ft_score = \"FT_Score\" in df.columns and df[\"FT_Score\"].notna().any()\n",
    "\n",
    "        if has_rag_score or has_ft_score:\n",
    "            cols = []\n",
    "            if has_rag_score: cols.append(\"RAG_Score\")\n",
    "            if has_ft_score: cols.append(\"FT_Score\")\n",
    "            \n",
    "            print(\"\\nðŸ“Š By Category (Score):\")\n",
    "            print(df.groupby(\"category\")[cols].mean())\n",
    "        else:\n",
    "             print(\"\\nâš ï¸ No LLM Judge Scores found (did you enable ENABLE_JUDGE?). Comparison by ROUGE:\")\n",
    "             print(df.groupby(\"category\")[[\"RAG_ROUGE\", \"FT_ROUGE\"]].mean())\n",
    "else:\n",
    "    print(\"âŒ Results files not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost_analysis",
   "metadata": {},
   "source": [
    "## 7. Cost Analysis (Bonus)\n",
    "**Scenario:** 500 users * 10 queries/day = 5,000 queries/day = 150,000 queries/month.\n",
    "\n",
    "**RAG (GPT-4o-mini):**\n",
    "- Input: ~1500 tokens (context) + 50 tokens (query) = 1550 tokens\n",
    "- Output: ~200 tokens\n",
    "- Price: $0.15/1M in, $0.60/1M out\n",
    "\n",
    "**Fine-Tuned (Self-Hosted GPU):**\n",
    "- Needs AWS g5.xlarge (A10G) -> ~$1.01/hr\n",
    "- Running 24/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cost_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’° Monthly Cost Comparison (150k queries):\n",
      "   RAG (GPT-4o-mini): $52.88\n",
      "   Fine-Tuned (g5.xlarge): $727.20\n",
      "\n",
      "   Winner for Cost: RAG\n"
     ]
    }
   ],
   "source": [
    "queries_per_month = 150000\n",
    "tokens_in = 1550\n",
    "tokens_out = 200\n",
    "\n",
    "# RAG Cost\n",
    "cost_rag_in = (queries_per_month * tokens_in / 1_000_000) * 0.15\n",
    "cost_rag_out = (queries_per_month * tokens_out / 1_000_000) * 0.60\n",
    "total_rag = cost_rag_in + cost_rag_out\n",
    "\n",
    "# FT Cost (AWS g5.xlarge on-demand)\n",
    "hourly_rate = 1.01\n",
    "total_ft = hourly_rate * 24 * 30\n",
    "\n",
    "print(f\"ðŸ’° Monthly Cost Comparison (150k queries):\")\n",
    "print(f\"   RAG (GPT-4o-mini): ${total_rag:.2f}\")\n",
    "print(f\"   Fine-Tuned (g5.xlarge): ${total_ft:.2f}\")\n",
    "print(f\"\\n   Winner for Cost: {'RAG' if total_rag < total_ft else 'Fine-Tuned'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluation Complete! Saved: final_results.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"data/final/final_results.csv\", index=False)\n",
    "print(\"âœ… Evaluation Complete! Saved: final_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Operation Ledger Mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
