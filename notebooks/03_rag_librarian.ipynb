{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31ff2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert OPENAI_API_KEY, \"Missing OPENAI_API_KEY in your .env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7fdebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = Path.cwd()\n",
    "if notebook_dir.name == \"notebooks\":\n",
    "    project_root = notebook_dir.parent\n",
    "else:\n",
    "    project_root = notebook_dir\n",
    "\n",
    "os.chdir(project_root)\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"ðŸ“‚ Working directory: {os.getcwd()}\")\n",
    "\n",
    "from src.services.llm_services import load_config\n",
    "\n",
    "config = load_config(\"src/config/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd69b4",
   "metadata": {},
   "source": [
    "Connect to Weaviate (v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b205e490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weaviate ready: True\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.connect import ConnectionParams\n",
    "from weaviate.classes.config import Property, DataType, Configure, Tokenization\n",
    "\n",
    "WEAVIATE_HTTP = \"http://localhost:8080\"\n",
    "WEAVIATE_GRPC_PORT = 50051\n",
    "\n",
    "client = weaviate.WeaviateClient(\n",
    "    connection_params=ConnectionParams.from_url(\n",
    "        WEAVIATE_HTTP,\n",
    "        grpc_port=WEAVIATE_GRPC_PORT\n",
    "    )\n",
    ")\n",
    "client.connect()\n",
    "print(\"Weaviate ready:\", client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d214e8",
   "metadata": {},
   "source": [
    "Create Schema (Weaviate Collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "865b1e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: FinancialChunk\n"
     ]
    }
   ],
   "source": [
    "COL = \"FinancialChunk\"\n",
    "\n",
    "if not client.collections.exists(COL):\n",
    "    client.collections.create(\n",
    "        name=COL,\n",
    "        properties=[\n",
    "            Property(name=\"text\", data_type=DataType.TEXT, tokenization=Tokenization.WORD),\n",
    "            Property(name=\"source\", data_type=DataType.TEXT),\n",
    "            Property(name=\"page\", data_type=DataType.INT),\n",
    "            Property(name=\"chunk_id\", data_type=DataType.TEXT),\n",
    "        ],\n",
    "        # Weaviate will create embeddings via OpenAI module\n",
    "        vectorizer_config=Configure.Vectorizer.text2vec_openai(\n",
    "            model=\"text-embedding-3-small\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "col = client.collections.get(COL)\n",
    "print(\"Collection:\", COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e7ba3",
   "metadata": {},
   "source": [
    "Load PDF (page-by-page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "623d329e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages with text: 142  / total pages: 142\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "PDF_PATH = Path(config[\"data_root\"] + \"/Uber_annual_report_2024.pdf\")\n",
    "assert PDF_PATH.exists(), f\"PDF not found: {PDF_PATH}\"\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.replace(\"\\x00\", \" \")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "reader = PdfReader(str(PDF_PATH))\n",
    "\n",
    "pages_text = []\n",
    "for i, page in enumerate(reader.pages, start=1):\n",
    "    txt = clean_text(page.extract_text() or \"\")\n",
    "    if txt:\n",
    "        pages_text.append({\"page\": i, \"text\": txt})\n",
    "\n",
    "print(\"Pages with text:\", len(pages_text), \" / total pages:\", len(reader.pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122fe48d",
   "metadata": {},
   "source": [
    "Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2395a5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks=1, avg=29, min=29, max=29\n",
      "chunks=1, avg=585, min=585, max=585\n",
      "chunks=2, avg=1381, min=1317, max=1446\n",
      "chunks=2, avg=1182, min=870, max=1495\n",
      "chunks=1, avg=1453, min=1453, max=1453\n",
      "chunks=4, avg=1206, min=491, max=1483\n",
      "chunks=2, avg=1351, min=1284, max=1418\n",
      "chunks=4, avg=1367, min=1246, max=1444\n",
      "chunks=4, avg=1441, min=1373, max=1486\n",
      "chunks=5, avg=1427, min=1328, max=1494\n",
      "chunks=4, avg=1331, min=975, max=1490\n",
      "chunks=5, avg=1272, min=632, max=1493\n",
      "chunks=4, avg=1465, min=1450, max=1497\n",
      "chunks=4, avg=1328, min=1032, max=1472\n",
      "chunks=5, avg=1439, min=1347, max=1488\n",
      "chunks=5, avg=1399, min=1308, max=1490\n",
      "chunks=6, avg=1271, min=393, max=1480\n",
      "chunks=6, avg=1330, min=652, max=1498\n",
      "chunks=6, avg=1291, min=534, max=1481\n",
      "chunks=6, avg=1286, min=669, max=1440\n",
      "chunks=6, avg=1312, min=804, max=1432\n",
      "chunks=6, avg=1226, min=250, max=1452\n",
      "chunks=4, avg=1310, min=980, max=1443\n",
      "chunks=5, avg=1339, min=895, max=1490\n",
      "chunks=6, avg=1213, min=190, max=1446\n",
      "chunks=6, avg=1244, min=390, max=1498\n",
      "chunks=6, avg=1316, min=762, max=1469\n",
      "chunks=6, avg=1300, min=535, max=1498\n",
      "chunks=5, avg=1401, min=1180, max=1492\n",
      "chunks=6, avg=1302, min=601, max=1490\n",
      "chunks=6, avg=1264, min=402, max=1494\n",
      "chunks=5, avg=1414, min=1370, max=1463\n",
      "chunks=5, avg=1290, min=674, max=1492\n",
      "chunks=6, avg=1245, min=310, max=1495\n",
      "chunks=6, avg=1292, min=398, max=1491\n",
      "chunks=5, avg=1351, min=806, max=1499\n",
      "chunks=5, avg=1341, min=930, max=1486\n",
      "chunks=6, avg=1334, min=783, max=1497\n",
      "chunks=6, avg=1264, min=272, max=1498\n",
      "chunks=6, avg=1282, min=639, max=1466\n",
      "chunks=6, avg=1396, min=1072, max=1485\n",
      "chunks=5, avg=1230, min=462, max=1488\n",
      "chunks=5, avg=1233, min=517, max=1457\n",
      "chunks=6, avg=1302, min=540, max=1485\n",
      "chunks=6, avg=1284, min=578, max=1471\n",
      "chunks=5, avg=1257, min=413, max=1486\n",
      "chunks=4, avg=1429, min=1374, max=1450\n",
      "chunks=5, avg=1228, min=472, max=1460\n",
      "chunks=5, avg=1268, min=656, max=1448\n",
      "chunks=5, avg=1229, min=482, max=1465\n",
      "chunks=4, avg=1161, min=270, max=1491\n",
      "chunks=3, avg=1028, min=187, max=1456\n",
      "chunks=4, avg=1406, min=1314, max=1451\n",
      "chunks=5, avg=1223, min=378, max=1449\n",
      "chunks=4, avg=1323, min=1064, max=1419\n",
      "chunks=4, avg=1351, min=995, max=1489\n",
      "chunks=2, avg=1430, min=1369, max=1492\n",
      "chunks=3, avg=1071, min=397, max=1434\n",
      "chunks=2, avg=1461, min=1425, max=1497\n",
      "chunks=3, avg=1176, min=753, max=1392\n",
      "chunks=3, avg=1210, min=673, max=1493\n",
      "chunks=3, avg=1321, min=1022, max=1498\n",
      "chunks=1, avg=1311, min=1311, max=1311\n",
      "chunks=3, avg=1237, min=834, max=1451\n",
      "chunks=3, avg=1377, min=1210, max=1463\n",
      "chunks=2, avg=1258, min=1018, max=1498\n",
      "chunks=5, avg=1313, min=817, max=1486\n",
      "chunks=4, avg=1380, min=1145, max=1491\n",
      "chunks=5, avg=1293, min=713, max=1495\n",
      "chunks=5, avg=1433, min=1368, max=1490\n",
      "chunks=5, avg=1374, min=1214, max=1444\n",
      "chunks=5, avg=1292, min=638, max=1487\n",
      "chunks=3, avg=1301, min=1113, max=1405\n",
      "chunks=1, avg=671, min=671, max=671\n",
      "chunks=5, avg=1353, min=948, max=1490\n",
      "chunks=5, avg=1362, min=1072, max=1497\n",
      "chunks=1, avg=137, min=137, max=137\n",
      "chunks=2, avg=1118, min=769, max=1468\n",
      "chunks=2, avg=921, min=349, max=1493\n",
      "chunks=1, avg=845, min=845, max=845\n",
      "chunks=1, avg=1471, min=1471, max=1471\n",
      "chunks=1, avg=1413, min=1413, max=1413\n",
      "chunks=2, avg=877, min=339, max=1416\n",
      "chunks=2, avg=1309, min=1129, max=1490\n",
      "chunks=1, avg=1243, min=1243, max=1243\n",
      "chunks=4, avg=1431, min=1332, max=1492\n",
      "chunks=4, avg=1257, min=696, max=1473\n",
      "chunks=5, avg=1191, min=208, max=1466\n",
      "chunks=5, avg=1372, min=1344, max=1409\n",
      "chunks=5, avg=1307, min=831, max=1473\n",
      "chunks=5, avg=1243, min=552, max=1431\n",
      "chunks=5, avg=1264, min=556, max=1490\n",
      "chunks=4, avg=1365, min=1046, max=1486\n",
      "chunks=5, avg=1207, min=262, max=1484\n",
      "chunks=5, avg=1292, min=725, max=1458\n",
      "chunks=5, avg=1353, min=939, max=1477\n",
      "chunks=5, avg=1311, min=825, max=1479\n",
      "chunks=3, avg=1117, min=444, max=1492\n",
      "chunks=2, avg=920, min=404, max=1437\n",
      "chunks=3, avg=1165, min=637, max=1491\n",
      "chunks=4, avg=1374, min=1077, max=1481\n",
      "chunks=3, avg=1439, min=1408, max=1472\n",
      "chunks=4, avg=1291, min=756, max=1485\n",
      "chunks=2, avg=1414, min=1339, max=1490\n",
      "chunks=1, avg=1459, min=1459, max=1459\n",
      "chunks=2, avg=1387, min=1324, max=1451\n",
      "chunks=2, avg=1355, min=1217, max=1494\n",
      "chunks=5, avg=1310, min=812, max=1493\n",
      "chunks=5, avg=1385, min=990, max=1495\n",
      "chunks=5, avg=1275, min=579, max=1478\n",
      "chunks=3, avg=1346, min=1053, max=1497\n",
      "chunks=3, avg=1300, min=997, max=1452\n",
      "chunks=2, avg=1094, min=732, max=1456\n",
      "chunks=3, avg=1256, min=966, max=1429\n",
      "chunks=3, avg=1372, min=1188, max=1493\n",
      "chunks=4, avg=1146, min=230, max=1490\n",
      "chunks=2, avg=816, min=212, max=1420\n",
      "chunks=2, avg=1137, min=777, max=1497\n",
      "chunks=4, avg=1153, min=404, max=1426\n",
      "chunks=4, avg=1176, min=237, max=1498\n",
      "chunks=3, avg=1198, min=640, max=1488\n",
      "chunks=2, avg=1325, min=1210, max=1441\n",
      "chunks=2, avg=1047, min=644, max=1450\n",
      "chunks=3, avg=1287, min=971, max=1465\n",
      "chunks=5, avg=1234, min=402, max=1466\n",
      "chunks=5, avg=1230, min=525, max=1452\n",
      "chunks=4, avg=1290, min=857, max=1459\n",
      "chunks=4, avg=1431, min=1350, max=1489\n",
      "chunks=4, avg=1298, min=828, max=1480\n",
      "chunks=5, avg=1321, min=728, max=1494\n",
      "chunks=2, avg=1311, min=1162, max=1461\n",
      "chunks=4, avg=1204, min=586, max=1454\n",
      "chunks=4, avg=1256, min=787, max=1437\n",
      "chunks=1, avg=1123, min=1123, max=1123\n",
      "chunks=3, avg=1365, min=1122, max=1489\n",
      "chunks=3, avg=1406, min=1246, max=1495\n",
      "chunks=2, avg=1467, min=1442, max=1493\n",
      "chunks=2, avg=1232, min=985, max=1480\n",
      "chunks=1, avg=24, min=24, max=24\n",
      "chunks=1, avg=37, min=37, max=37\n",
      "chunks=2, avg=1131, min=783, max=1479\n",
      "chunks=1, avg=79, min=79, max=79\n",
      "Total chunks: 537\n",
      "Sample: 1-0 On Our Way\n",
      "2024 ANNUAL REPORT\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def create_chunks(text: str, chunk_size: int = 1500, chunk_overlap: int = 200) -> List[str]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    # optional stats\n",
    "    if chunks:\n",
    "        print(f\"chunks={len(chunks)}, avg={sum(len(c) for c in chunks)//len(chunks)}, \"\n",
    "              f\"min={min(len(c) for c in chunks)}, max={max(len(c) for c in chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "all_chunks = []\n",
    "for p in pages_text:\n",
    "    chunks = create_chunks(p[\"text\"], 1500, 200)\n",
    "    for j, ch in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"text\": ch,\n",
    "            \"source\": PDF_PATH.name,\n",
    "            \"page\": p[\"page\"],\n",
    "            \"chunk_id\": f\"{p['page']}-{j}\"\n",
    "        })\n",
    "\n",
    "print(\"Total chunks:\", len(all_chunks))\n",
    "print(\"Sample:\", all_chunks[0][\"chunk_id\"], all_chunks[0][\"text\"][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86ede6",
   "metadata": {},
   "source": [
    "Index into Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24df7cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 537/537 [00:14<00:00, 38.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "BATCH = 64\n",
    "\n",
    "with col.batch.dynamic() as b:\n",
    "    for obj in tqdm(all_chunks, desc=\"Indexing\"):\n",
    "        b.add_object(properties=obj)\n",
    "\n",
    "print(\"Indexed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af1fd6",
   "metadata": {},
   "source": [
    "Retrieval: BM25 + Dense Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d56c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_bm25(q: str, k: int = 25):\n",
    "    res = col.query.bm25(query=q, limit=k)\n",
    "    return res.objects  # ranked\n",
    "\n",
    "def retrieve_vector(q: str, k: int = 25):\n",
    "    res = col.query.near_text(query=q, limit=k)\n",
    "    return res.objects  # ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474848c1",
   "metadata": {},
   "source": [
    "RRF fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eede455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_fuse(lists: List[List[Any]], rrf_k: int = 60, top_n: int = 30) -> List[Any]:\n",
    "    scores = defaultdict(float)\n",
    "\n",
    "    for lst in lists:\n",
    "        for rank, obj in enumerate(lst, start=1):\n",
    "            scores[str(obj.uuid)] += 1.0 / (rrf_k + rank)\n",
    "\n",
    "    ranked_ids = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    wanted = {rid for rid, _ in ranked_ids}\n",
    "\n",
    "    by_id = {}\n",
    "    for lst in lists:\n",
    "        for obj in lst:\n",
    "            sid = str(obj.uuid)\n",
    "            if sid in wanted and sid not in by_id:\n",
    "                by_id[sid] = obj\n",
    "\n",
    "    return [by_id[rid] for rid, _ in ranked_ids if rid in by_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2120e",
   "metadata": {},
   "source": [
    "Cross-Encoder rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "931b5794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e6da07ce2f42da99d9bc4e678e7a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Bootcamps\\AEE 2026 Jan\\Mini Projects\\Operation Ledger Mind\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\udaya\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e552972ecf084c20ab0f93b02c923e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9ea7d158dc49ac980b61c7bf08115e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f457694e6d6c471ab4b0f52079d25f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24648d001deb4e4ab0e71b6301b38ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240c616139b24fe9a7856d1bdbc28dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c2e91a3a8d4a0dacfc93ed79ab63fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
    "\n",
    "def cross_encoder_rerank(question: str, candidates: List[Any], top_n: int = 8) -> List[Any]:\n",
    "    pairs = [(question, c.properties[\"text\"]) for c in candidates]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [c for c, _ in ranked[:top_n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258099c",
   "metadata": {},
   "source": [
    "query_librarian(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0378b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "oa = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def build_context(objs: List[Any]) -> str:\n",
    "    out = []\n",
    "    for i, o in enumerate(objs, start=1):\n",
    "        src = o.properties.get(\"source\", \"unknown\")\n",
    "        page = o.properties.get(\"page\", None)\n",
    "        text = o.properties.get(\"text\", \"\")\n",
    "        out.append(f\"[{i}] ({src}, p.{page})\\n{text}\")\n",
    "    return \"\\n\\n\".join(out)\n",
    "\n",
    "def query_librarian(question: str,\n",
    "                    k_retrieve: int = 25,\n",
    "                    fused_n: int = 30,\n",
    "                    final_n: int = 8,\n",
    "                    rrf_k: int = 60,\n",
    "                    model: str = \"gpt-4o-mini\") -> Dict[str, Any]:\n",
    "    t0 = time.time()\n",
    "\n",
    "    bm25 = retrieve_bm25(question, k=k_retrieve)\n",
    "    vec  = retrieve_vector(question, k=k_retrieve)\n",
    "\n",
    "    fused = rrf_fuse([bm25, vec], rrf_k=rrf_k, top_n=fused_n)\n",
    "    top = cross_encoder_rerank(question, fused, top_n=final_n)\n",
    "\n",
    "    context = build_context(top)\n",
    "\n",
    "    system = (\n",
    "        \"You are The Librarian for financial PDFs. \"\n",
    "        \"Answer ONLY using the provided context. \"\n",
    "        \"If insufficient, say you don't have enough information. \"\n",
    "        \"Include page citations like (p. X) for claims.\"\n",
    "    )\n",
    "\n",
    "    user = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    resp = oa.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": system},\n",
    "                  {\"role\": \"user\", \"content\": user}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"answer\": resp.choices[0].message.content,\n",
    "        \"latency_ms\": int((time.time() - t0) * 1000),\n",
    "        \"evidence\": [\n",
    "            {\"source\": o.properties.get(\"source\"),\n",
    "             \"page\": o.properties.get(\"page\"),\n",
    "             \"chunk_id\": o.properties.get(\"chunk_id\")}\n",
    "            for o in top\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02843398",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59155bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Form 10-K is mentioned multiple times in the document. One relevant part states: \"We undertake no obligation to update any forward-looking statements made in this Annual Report on Form 10-K to reflect events or circumstances after the date of this Annual Report on Form 10-K or to reflect new information, actual results, revised expectations, or the occurrence of unanticipated events, except as required by law\" (p. 7).\n",
      "latency_ms: 8984\n",
      "evidence: [{'source': 'Uber_annual_report_2024.pdf', 'page': 7, 'chunk_id': '7-1'}, {'source': 'Uber_annual_report_2024.pdf', 'page': 52, 'chunk_id': '52-2'}, {'source': 'Uber_annual_report_2024.pdf', 'page': 134, 'chunk_id': '134-0'}, {'source': 'Uber_annual_report_2024.pdf', 'page': 13, 'chunk_id': '13-2'}, {'source': 'Uber_annual_report_2024.pdf', 'page': 52, 'chunk_id': '52-1'}, {'source': 'Uber_annual_report_2024.pdf', 'page': 7, 'chunk_id': '7-0'}, {'source': 'Uber_annual_report_2024.pdf', 'page': 68, 'chunk_id': '68-3'}, {'source': 'Uber_annual_report_2024.pdf', 'page': 4, 'chunk_id': '4-1'}]\n"
     ]
    }
   ],
   "source": [
    "out = query_librarian(\"Where is Form 10-K mentioned? Quote the relevant part.\")\n",
    "print(out[\"answer\"])\n",
    "print(\"latency_ms:\", out[\"latency_ms\"])\n",
    "print(\"evidence:\", out[\"evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4491a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Operation Ledger Mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
